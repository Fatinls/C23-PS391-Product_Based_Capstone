{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cakalang_surabayakota</th>\n",
       "      <th>cakalang_malangkota</th>\n",
       "      <th>cakalang_banyuwangikab</th>\n",
       "      <th>cakalang_blitarkab</th>\n",
       "      <th>cakalang_jombangkab</th>\n",
       "      <th>cakalang_madiunkab</th>\n",
       "      <th>cakalang_malangkab</th>\n",
       "      <th>cakalang_mojokertokab</th>\n",
       "      <th>cakalang_nganjukkab</th>\n",
       "      <th>cakalang_pasuruankab</th>\n",
       "      <th>...</th>\n",
       "      <th>tuna_lumajangkab</th>\n",
       "      <th>tuna_madiunkab</th>\n",
       "      <th>tuna_malangkab</th>\n",
       "      <th>tuna_mojokertokab</th>\n",
       "      <th>tuna_nganjukkab</th>\n",
       "      <th>tuna_pamekasankab</th>\n",
       "      <th>tuna_pasuruankab</th>\n",
       "      <th>tuna_sidoarjokab</th>\n",
       "      <th>tuna_batukota</th>\n",
       "      <th>tuna_madiunkota</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanggal</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>29500</td>\n",
       "      <td>28800</td>\n",
       "      <td>28000</td>\n",
       "      <td>25000</td>\n",
       "      <td>27500</td>\n",
       "      <td>30000</td>\n",
       "      <td>30200</td>\n",
       "      <td>34500</td>\n",
       "      <td>29500</td>\n",
       "      <td>24833</td>\n",
       "      <td>...</td>\n",
       "      <td>31000</td>\n",
       "      <td>32000</td>\n",
       "      <td>34400</td>\n",
       "      <td>37000</td>\n",
       "      <td>35666</td>\n",
       "      <td>70000</td>\n",
       "      <td>29666</td>\n",
       "      <td>32000</td>\n",
       "      <td>38500</td>\n",
       "      <td>34333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>29500</td>\n",
       "      <td>28800</td>\n",
       "      <td>28000</td>\n",
       "      <td>25000</td>\n",
       "      <td>27500</td>\n",
       "      <td>30000</td>\n",
       "      <td>31160</td>\n",
       "      <td>34500</td>\n",
       "      <td>29500</td>\n",
       "      <td>24833</td>\n",
       "      <td>...</td>\n",
       "      <td>31000</td>\n",
       "      <td>32000</td>\n",
       "      <td>34800</td>\n",
       "      <td>37000</td>\n",
       "      <td>35666</td>\n",
       "      <td>70000</td>\n",
       "      <td>29666</td>\n",
       "      <td>32000</td>\n",
       "      <td>38250</td>\n",
       "      <td>34333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-26</th>\n",
       "      <td>29500</td>\n",
       "      <td>28800</td>\n",
       "      <td>28000</td>\n",
       "      <td>25000</td>\n",
       "      <td>27500</td>\n",
       "      <td>30000</td>\n",
       "      <td>30600</td>\n",
       "      <td>34500</td>\n",
       "      <td>29500</td>\n",
       "      <td>24833</td>\n",
       "      <td>...</td>\n",
       "      <td>31500</td>\n",
       "      <td>32000</td>\n",
       "      <td>35000</td>\n",
       "      <td>37000</td>\n",
       "      <td>35666</td>\n",
       "      <td>70000</td>\n",
       "      <td>29666</td>\n",
       "      <td>32000</td>\n",
       "      <td>38250</td>\n",
       "      <td>34333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-27</th>\n",
       "      <td>29500</td>\n",
       "      <td>28800</td>\n",
       "      <td>28000</td>\n",
       "      <td>25000</td>\n",
       "      <td>27500</td>\n",
       "      <td>30000</td>\n",
       "      <td>30600</td>\n",
       "      <td>34500</td>\n",
       "      <td>29500</td>\n",
       "      <td>24833</td>\n",
       "      <td>...</td>\n",
       "      <td>31000</td>\n",
       "      <td>32000</td>\n",
       "      <td>35000</td>\n",
       "      <td>37000</td>\n",
       "      <td>35666</td>\n",
       "      <td>70000</td>\n",
       "      <td>29666</td>\n",
       "      <td>32000</td>\n",
       "      <td>38250</td>\n",
       "      <td>34333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>29500</td>\n",
       "      <td>28800</td>\n",
       "      <td>28000</td>\n",
       "      <td>25000</td>\n",
       "      <td>27500</td>\n",
       "      <td>30000</td>\n",
       "      <td>30600</td>\n",
       "      <td>34500</td>\n",
       "      <td>29500</td>\n",
       "      <td>24833</td>\n",
       "      <td>...</td>\n",
       "      <td>31000</td>\n",
       "      <td>32000</td>\n",
       "      <td>35400</td>\n",
       "      <td>37000</td>\n",
       "      <td>35666</td>\n",
       "      <td>70000</td>\n",
       "      <td>29666</td>\n",
       "      <td>32000</td>\n",
       "      <td>38750</td>\n",
       "      <td>34333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cakalang_surabayakota  cakalang_malangkota  \\\n",
       "tanggal                                                  \n",
       "2018-05-24                  29500                28800   \n",
       "2018-05-25                  29500                28800   \n",
       "2018-05-26                  29500                28800   \n",
       "2018-05-27                  29500                28800   \n",
       "2018-05-28                  29500                28800   \n",
       "\n",
       "            cakalang_banyuwangikab  cakalang_blitarkab  cakalang_jombangkab  \\\n",
       "tanggal                                                                       \n",
       "2018-05-24                   28000               25000                27500   \n",
       "2018-05-25                   28000               25000                27500   \n",
       "2018-05-26                   28000               25000                27500   \n",
       "2018-05-27                   28000               25000                27500   \n",
       "2018-05-28                   28000               25000                27500   \n",
       "\n",
       "            cakalang_madiunkab  cakalang_malangkab  cakalang_mojokertokab  \\\n",
       "tanggal                                                                     \n",
       "2018-05-24               30000               30200                  34500   \n",
       "2018-05-25               30000               31160                  34500   \n",
       "2018-05-26               30000               30600                  34500   \n",
       "2018-05-27               30000               30600                  34500   \n",
       "2018-05-28               30000               30600                  34500   \n",
       "\n",
       "            cakalang_nganjukkab  cakalang_pasuruankab  ...  tuna_lumajangkab  \\\n",
       "tanggal                                                ...                     \n",
       "2018-05-24                29500                 24833  ...             31000   \n",
       "2018-05-25                29500                 24833  ...             31000   \n",
       "2018-05-26                29500                 24833  ...             31500   \n",
       "2018-05-27                29500                 24833  ...             31000   \n",
       "2018-05-28                29500                 24833  ...             31000   \n",
       "\n",
       "            tuna_madiunkab  tuna_malangkab  tuna_mojokertokab  \\\n",
       "tanggal                                                         \n",
       "2018-05-24           32000           34400              37000   \n",
       "2018-05-25           32000           34800              37000   \n",
       "2018-05-26           32000           35000              37000   \n",
       "2018-05-27           32000           35000              37000   \n",
       "2018-05-28           32000           35400              37000   \n",
       "\n",
       "            tuna_nganjukkab  tuna_pamekasankab  tuna_pasuruankab  \\\n",
       "tanggal                                                            \n",
       "2018-05-24            35666              70000             29666   \n",
       "2018-05-25            35666              70000             29666   \n",
       "2018-05-26            35666              70000             29666   \n",
       "2018-05-27            35666              70000             29666   \n",
       "2018-05-28            35666              70000             29666   \n",
       "\n",
       "            tuna_sidoarjokab  tuna_batukota  tuna_madiunkota  \n",
       "tanggal                                                       \n",
       "2018-05-24             32000          38500            34333  \n",
       "2018-05-25             32000          38250            34333  \n",
       "2018-05-26             32000          38250            34333  \n",
       "2018-05-27             32000          38250            34333  \n",
       "2018-05-28             32000          38750            34333  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'CakalangTuna (1).csv', sep=',', header=0, low_memory=False, index_col=['tanggal'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=np.array(df[:][:180])\n",
    "# df_reshaped = np.reshape(df1, (1, 180, 27))\n",
    "# print(df_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df\n",
    "scalers={}\n",
    "for i in df.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    s_s = scaler.fit_transform(df1[i].values.reshape(-1,1))\n",
    "    s_s=np.reshape(s_s,len(s_s))\n",
    "    scalers['scaler_'+ i] = scaler\n",
    "    df1[i]=s_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df1[:math.floor(0.75*1827)], df1[math.floor(0.75*1827):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train_df\n",
    "# scalers={}\n",
    "# for i in train_df.columns:\n",
    "#     scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#     s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
    "#     s_s=np.reshape(s_s,len(s_s))\n",
    "#     scalers['scaler_'+ i] = scaler\n",
    "#     train[i]=s_s\n",
    "# test = test_df\n",
    "# for i in train_df.columns:\n",
    "#     scaler = scalers['scaler_'+i]\n",
    "#     s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
    "#     s_s=np.reshape(s_s,len(s_s))\n",
    "#     scalers['scaler_'+i] = scaler\n",
    "#     test[i]=s_s\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series(series, n_past, n_future):\n",
    "  #\n",
    "  # n_past ==> no of past observations\n",
    "  #\n",
    "  # n_future ==> no of future observations \n",
    "  #\n",
    "  X, y = list(), list()\n",
    "  for window_start in range(len(series)):\n",
    "    past_end = window_start + n_past\n",
    "    future_end = past_end + n_future\n",
    "    if future_end > len(series):\n",
    "      break\n",
    "    # slicing the past and future parts of the window\n",
    "    past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "    X.append(past)\n",
    "    y.append(future)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_past = 180\n",
    "n_future =  30\n",
    "n_features = 27\n",
    "X_train, y_train = split_series(train.values,n_past, n_future)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
    "X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))\n",
    "# print(X_test.shape[0],X_test.shape[1],X_test.shape[2])\n",
    "# print(y_test.shape[0],y_test.shape[1],y_test.shape[2])\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_uncompiled_model():\n",
    "\n",
    "#     ### START CODE HERE\n",
    "    \n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Conv1D(filters=8, kernel_size=4,\n",
    "#                                strides=1,\n",
    "#                                activation='relu',\n",
    "#                                padding='causal',\n",
    "#                                input_shape=[None, 27]),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "#         tf.keras.layers.LSTM(8, return_sequences=True),\n",
    "#         tf.keras.layers.LSTM(4, return_sequences=True),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "#         tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(8, activation='relu')),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "#         tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(27))\n",
    "#     ])\n",
    "\n",
    "    \n",
    "#     ### END CODE HERE\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(X_train, y_train):\n",
    "    \n",
    "#     model = create_uncompiled_model()\n",
    "    \n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "    \n",
    "#     ### START CODE HERE\n",
    "    \n",
    "#     # Select your optimizer\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "#     # Compile the model passing in the appropriate loss\n",
    "#     model.compile(loss=tf.keras.losses.Huber(),\n",
    "#                   optimizer=optimizer, \n",
    "#                   metrics=[\"mae\"]) \n",
    "    \n",
    "#     ### END CODE HERE\n",
    "    \n",
    "#     history = model.fit(X_train, y_train, epochs=100, callbacks=[lr_schedule])\n",
    "    \n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lr_history = adjust_learning_rate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 27)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 180, 16)      1744        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 180, 16),    2112        ['conv1d[0][0]']                 \n",
      "                                 (None, 16),                                                      \n",
      "                                 (None, 16)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 8),          800         ['lstm[0][0]']                   \n",
      "                                 (None, 8),                                                       \n",
      "                                 (None, 8)]                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 8)            0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 30, 8)        0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 30, 16)       1600        ['repeat_vector[0][0]',          \n",
      "                                                                  'lstm[0][1]',                   \n",
      "                                                                  'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 30, 8)        800         ['lstm_2[0][0]',                 \n",
      "                                                                  'lstm_1[0][1]',                 \n",
      "                                                                  'lstm_1[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 30, 27)      243         ['lstm_3[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,299\n",
      "Trainable params: 7,299\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# E2D2\n",
    "# n_features ==> no of features at each timestep in the data.\n",
    "#\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "\n",
    "# Add Conv1D layer\n",
    "encoder_cnn = tf.keras.layers.Conv1D(16, kernel_size=4, activation='relu', padding='causal')(encoder_inputs)\n",
    "\n",
    "encoder_l1 = tf.keras.layers.LSTM(16, return_sequences=True, return_state=True)\n",
    "encoder_outputs1 = encoder_l1(encoder_cnn)\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "encoder_l2 = tf.keras.layers.LSTM(8, return_state=True)\n",
    "encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "encoder_states2 = encoder_outputs2[1:]\n",
    "\n",
    "dropout_rate = 0.4\n",
    "dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(dropout_layer(encoder_outputs2[0]))\n",
    "\n",
    "decoder_l1 = tf.keras.layers.LSTM(16, return_sequences=True)(decoder_inputs, initial_state=encoder_states1)\n",
    "decoder_l2 = tf.keras.layers.LSTM(8, return_sequences=True)(decoder_l1, initial_state=encoder_states2)\n",
    "decoder_outputs2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l2)\n",
    "\n",
    "model_e2d2 = tf.keras.models.Model(encoder_inputs, decoder_outputs2)\n",
    "\n",
    "model_e2d2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 12s 151ms/step - loss: 0.0276 - mae: 0.1681 - val_loss: 0.0690 - val_mae: 0.3044\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.0114 - mae: 0.1117 - val_loss: 0.0667 - val_mae: 0.2949\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0105 - mae: 0.1077 - val_loss: 0.0587 - val_mae: 0.2756\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0081 - mae: 0.0925 - val_loss: 0.0550 - val_mae: 0.2677\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0068 - mae: 0.0796 - val_loss: 0.0551 - val_mae: 0.2701\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0063 - mae: 0.0756 - val_loss: 0.0539 - val_mae: 0.2675\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0058 - mae: 0.0711 - val_loss: 0.0533 - val_mae: 0.2642\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0055 - mae: 0.0692 - val_loss: 0.0517 - val_mae: 0.2597\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0051 - mae: 0.0672 - val_loss: 0.0537 - val_mae: 0.2670\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0048 - mae: 0.0658 - val_loss: 0.0543 - val_mae: 0.2679\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0052 - mae: 0.0687 - val_loss: 0.0547 - val_mae: 0.2695\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0052 - mae: 0.0687 - val_loss: 0.0552 - val_mae: 0.2707\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0051 - mae: 0.0672 - val_loss: 0.0543 - val_mae: 0.2692\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0044 - mae: 0.0627 - val_loss: 0.0507 - val_mae: 0.2599\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0044 - mae: 0.0631 - val_loss: 0.0554 - val_mae: 0.2711\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0042 - mae: 0.0609 - val_loss: 0.0529 - val_mae: 0.2651\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0040 - mae: 0.0593 - val_loss: 0.0533 - val_mae: 0.2647\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0039 - mae: 0.0575 - val_loss: 0.0527 - val_mae: 0.2632\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0038 - mae: 0.0570 - val_loss: 0.0525 - val_mae: 0.2637\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0037 - mae: 0.0558 - val_loss: 0.0525 - val_mae: 0.2631\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0038 - mae: 0.0559 - val_loss: 0.0512 - val_mae: 0.2597\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0043 - mae: 0.0597 - val_loss: 0.0507 - val_mae: 0.2579\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0038 - mae: 0.0554 - val_loss: 0.0512 - val_mae: 0.2593\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0037 - mae: 0.0546 - val_loss: 0.0512 - val_mae: 0.2591\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0036 - mae: 0.0537 - val_loss: 0.0525 - val_mae: 0.2630\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0038 - mae: 0.0548 - val_loss: 0.0555 - val_mae: 0.2725\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0039 - mae: 0.0561 - val_loss: 0.0524 - val_mae: 0.2631\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0035 - mae: 0.0533 - val_loss: 0.0517 - val_mae: 0.2606\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0036 - mae: 0.0536 - val_loss: 0.0514 - val_mae: 0.2590\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0040 - mae: 0.0557 - val_loss: 0.0536 - val_mae: 0.2658\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0037 - mae: 0.0541 - val_loss: 0.0526 - val_mae: 0.2635\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0035 - mae: 0.0524 - val_loss: 0.0499 - val_mae: 0.2556\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0034 - mae: 0.0517 - val_loss: 0.0518 - val_mae: 0.2601\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0034 - mae: 0.0520 - val_loss: 0.0524 - val_mae: 0.2623\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0035 - mae: 0.0529 - val_loss: 0.0503 - val_mae: 0.2575\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0034 - mae: 0.0527 - val_loss: 0.0521 - val_mae: 0.2615\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0035 - mae: 0.0525 - val_loss: 0.0510 - val_mae: 0.2589\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0033 - mae: 0.0510 - val_loss: 0.0508 - val_mae: 0.2574\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0033 - mae: 0.0514 - val_loss: 0.0507 - val_mae: 0.2575\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0033 - mae: 0.0513 - val_loss: 0.0532 - val_mae: 0.2633\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0033 - mae: 0.0512 - val_loss: 0.0523 - val_mae: 0.2612\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0033 - mae: 0.0513 - val_loss: 0.0511 - val_mae: 0.2597\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0033 - mae: 0.0512 - val_loss: 0.0507 - val_mae: 0.2574\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0033 - mae: 0.0510 - val_loss: 0.0508 - val_mae: 0.2584\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0034 - mae: 0.0513 - val_loss: 0.0507 - val_mae: 0.2573\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0033 - mae: 0.0511 - val_loss: 0.0514 - val_mae: 0.2591\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0034 - mae: 0.0516 - val_loss: 0.0515 - val_mae: 0.2603\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0033 - mae: 0.0509 - val_loss: 0.0514 - val_mae: 0.2591\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0032 - mae: 0.0501 - val_loss: 0.0502 - val_mae: 0.2558\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0032 - mae: 0.0505 - val_loss: 0.0503 - val_mae: 0.2571\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "# model_e1d1.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "# history_e1d1=model_e1d1.fit(X_train,y_train,epochs=25,validation_data=(X_test,y_test),batch_size=32,verbose=0,callbacks=[reduce_lr])\n",
    "model_e2d2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0089), loss=tf.keras.losses.Huber(),metrics=[\"mae\"])\n",
    "history_e2d2=model_e2d2.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=32,verbose=1)#,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 2s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_e2d2=model_e2d2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(train.columns):\n",
    "    scaler = scalers['scaler_'+i]\n",
    "    pred_e2d2[:,:,index]=scaler.inverse_transform(pred_e2d2[:,:,index]).astype(int)\n",
    "    y_train[:,:,index]=scaler.inverse_transform(y_train[:,:,index]).astype(int)\n",
    "    y_test[:,:,index]=scaler.inverse_transform(y_test[:,:,index]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[35536. 32504. 29702. ... 35054. 38593. 35499.]\n",
      "  [35721. 32488. 29815. ... 35053. 38442. 35499.]\n",
      "  [35688. 32467. 29812. ... 35067. 38533. 35497.]\n",
      "  ...\n",
      "  [35649. 32423. 29822. ... 35057. 38613. 35497.]\n",
      "  [35635. 32423. 29815. ... 35055. 38617. 35498.]\n",
      "  [35617. 32424. 29807. ... 35054. 38621. 35500.]]\n",
      "\n",
      " [[35535. 32500. 29705. ... 35052. 38592. 35498.]\n",
      "  [35718. 32484. 29815. ... 35051. 38440. 35497.]\n",
      "  [35687. 32464. 29813. ... 35065. 38532. 35495.]\n",
      "  ...\n",
      "  [35648. 32423. 29822. ... 35056. 38613. 35497.]\n",
      "  [35634. 32423. 29815. ... 35055. 38617. 35498.]\n",
      "  [35617. 32424. 29807. ... 35054. 38621. 35500.]]\n",
      "\n",
      " [[35536. 32495. 29708. ... 35050. 38593. 35497.]\n",
      "  [35716. 32479. 29816. ... 35048. 38438. 35495.]\n",
      "  [35685. 32460. 29814. ... 35063. 38530. 35494.]\n",
      "  ...\n",
      "  [35648. 32423. 29822. ... 35056. 38613. 35497.]\n",
      "  [35634. 32423. 29815. ... 35055. 38617. 35498.]\n",
      "  [35616. 32423. 29807. ... 35053. 38621. 35500.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[35653. 32649. 29690. ... 35142. 38678. 35555.]\n",
      "  [35809. 32637. 29795. ... 35136. 38540. 35566.]\n",
      "  [35764. 32604. 29794. ... 35149. 38627. 35558.]\n",
      "  ...\n",
      "  [35690. 32437. 29834. ... 35068. 38611. 35498.]\n",
      "  [35688. 32435. 29834. ... 35067. 38611. 35498.]\n",
      "  [35685. 32434. 29834. ... 35066. 38611. 35497.]]\n",
      "\n",
      " [[35654. 32650. 29691. ... 35142. 38678. 35555.]\n",
      "  [35808. 32637. 29795. ... 35136. 38540. 35566.]\n",
      "  [35764. 32604. 29794. ... 35149. 38627. 35558.]\n",
      "  ...\n",
      "  [35690. 32437. 29834. ... 35068. 38611. 35498.]\n",
      "  [35688. 32435. 29834. ... 35067. 38611. 35498.]\n",
      "  [35685. 32434. 29834. ... 35066. 38611. 35497.]]\n",
      "\n",
      " [[35654. 32650. 29691. ... 35142. 38679. 35555.]\n",
      "  [35808. 32637. 29795. ... 35136. 38540. 35566.]\n",
      "  [35764. 32604. 29794. ... 35149. 38627. 35558.]\n",
      "  ...\n",
      "  [35690. 32437. 29834. ... 35068. 38611. 35498.]\n",
      "  [35688. 32435. 29834. ... 35067. 38611. 35498.]\n",
      "  [35685. 32434. 29834. ... 35066. 38611. 35497.]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(pred_e2d2)\n",
    "# for index,i in enumerate(train.columns):\n",
    "#   print(i)\n",
    "#   for j in range(1,31):\n",
    "#     print(\"Day \",j,\":\",pred_e2d2[-1,j-1,index])\n",
    "#     print(\"MAE-E2D2 : \",mean_absolute_error(y_test[:,j-1,index],pred_e2d2[:,j-1,index]))\n",
    "#   print()\n",
    "# 1,3,2,...\n",
    "# 1,2,3,...\n",
    "# 1,2,3,...\n",
    "\n",
    "# [[12,34,56,...],\n",
    "#  [13,15,16,...]],\n",
    "# [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# saved_model_path = \"model/\"\n",
    "\n",
    "# tf.saved_model.save(model_e2d2, saved_model_path)\n",
    "# tfjs.converters.convert_tf_saved_model(saved_model_path, saved_model_path)\n",
    "import time\n",
    "saved_model_path = \"model/{}.h5\".format(int(time.time()))\n",
    "\n",
    "model_e2d2.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_uncompiled_model():\n",
    "\n",
    "#     ### START CODE HERE\n",
    "    \n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Conv1D(filters=8, kernel_size=4,\n",
    "#                                strides=1,\n",
    "#                                activation='relu',\n",
    "#                                padding='causal',\n",
    "#                                input_shape=[n_past, n_features]),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "#         # tf.keras.layers.LSTM(512, return_sequences=True),\n",
    "#         # # tf.keras.layers.LSTM(256),\n",
    "#         # tf.keras.layers.Conv1D(filters=512, kernel_size=4,\n",
    "#         #                        strides=1,\n",
    "#         #                        activation='relu',\n",
    "#         #                        padding='causal'),\n",
    "#         # tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "#         # # tf.keras.layers.LSTM(128),\n",
    "#         # tf.keras.layers.Conv1D(filters=256, kernel_size=4,\n",
    "#         #                        strides=1,\n",
    "#         #                        activation='relu',\n",
    "#         #                        padding='causal'),\n",
    "#         # tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "#         # # tf.keras.layers.LSTM(64),\n",
    "#         # tf.keras.layers.Conv1D(filters=512, kernel_size=4,\n",
    "#         #                        strides=1,\n",
    "#         #                        activation='relu',\n",
    "#         #                        padding='causal'),\n",
    "#         # tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "#         # # tf.keras.layers.LSTM(128),\n",
    "#         # tf.keras.layers.Conv1D(filters=64, kernel_size=4,\n",
    "#         #                        strides=1,\n",
    "#         #                        activation='relu',\n",
    "#         #                        padding='causal'),\n",
    "#         # tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "#         # # tf.keras.layers.LSTM(64),\n",
    "#         # tf.keras.layers.Conv1D(filters=32, kernel_size=4,\n",
    "#         #                        strides=1,\n",
    "#         #                        activation='relu',\n",
    "#         #                        padding='causal'),\n",
    "#         tf.keras.layers.LSTM(8, return_sequences=True),\n",
    "#         tf.keras.layers.LSTM(4),\n",
    "#         tf.keras.layers.Dropout(0.4),                                        \n",
    "#         # tf.keras.layers.Dense(512, activation='relu'),\n",
    "#         # tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         # tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         # tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.Dense(8, activation='relu'),\n",
    "#         tf.keras.layers.Dropout(0.4),                \n",
    "#         tf.keras.layers.Dense(27)\n",
    "#     ]) \n",
    "    \n",
    "#     ### END CODE HERE\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(X_train, y_train):\n",
    "    \n",
    "#     model = create_uncompiled_model()\n",
    "    \n",
    "#     lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
    "    \n",
    "#     ### START CODE HERE\n",
    "    \n",
    "#     # Select your optimizer\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "#     # Compile the model passing in the appropriate loss\n",
    "#     model.compile(loss=tf.keras.losses.Huber(),\n",
    "#                   optimizer=optimizer, \n",
    "#                   metrics=[\"mae\"]) \n",
    "    \n",
    "#     ### END CODE HERE\n",
    "    \n",
    "#     history = model.fit(X_train, y_train, epochs=100, callbacks=[lr_schedule])\n",
    "    \n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_history = adjust_learning_rate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
